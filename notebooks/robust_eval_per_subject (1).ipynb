{"cells":[{"cell_type":"markdown","metadata":{"id":"NbsRlknDg95u"},"source":["# Robust, Leakageâ€‘Free Evaluation with Perâ€‘Subject Normalization\n","\n","This notebook implements **group-aware** training/validation to avoid leakage when doing **per-subject normalization**. It includes:\n","\n","- GroupKFold and **Leave-One-Subject-Out (LOSO)** evaluation\n","- Two scaling modes for X: **per-subject** (fit on train-only stats per subject) or **global** (fit on train only)\n","- Y preprocessing: **Imputer â†’ StandardScaler â†’ PCA** (fit on train only)\n","- Model baselines: **RidgeCV** and **RandomForestRegressor** (easily replaceable)\n","- Optional **log1p** transform for power-like features\n","- Clear metrics: RÂ², MAE, Pearson r (macro over targets)\n","\n","ðŸ‘‰ *How to use:* Replace the **Data Loading** cell to construct `X`, `Y`, and `subjects` (a 1D array-like subject id per row). Then run all cells.\n"],"id":"NbsRlknDg95u"},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true,"id":"ImMpP5b5g95v","executionInfo":{"status":"ok","timestamp":1760274188724,"user_tz":-180,"elapsed":3553,"user":{"displayName":"Mahler David","userId":"01405886190187025860"}}},"outputs":[],"source":["# !pip install numpy pandas scikit-learn scipy\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import GroupKFold\n","from sklearn.linear_model import RidgeCV\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import r2_score, mean_absolute_error\n","from sklearn.impute import SimpleImputer\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from scipy.stats import pearsonr\n","from typing import Tuple, Dict, Any\n","import warnings\n","warnings.filterwarnings('ignore')\n"],"id":"ImMpP5b5g95v"},{"cell_type":"markdown","metadata":{"id":"D8by6Fi4g95w"},"source":["## Data Loading (Replace This Cell)\n","You must define:\n","- `X`: shape (n_samples, n_features), `pd.DataFrame` or `np.ndarray`\n","- `Y`: shape (n_samples, n_targets), `pd.DataFrame` or `np.ndarray`\n","- `subjects`: shape (n_samples,), array-like subject IDs (int/str)\n","\n","Optionally, set `FEATURES_ARE_POWERS = True` to apply `log1p` to X.\n"],"id":"D8by6Fi4g95w"},{"cell_type":"code","source":[],"metadata":{"id":"TN1T4l1YhKUR"},"id":"TN1T4l1YhKUR","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"lDZx7Nalg95w"},"outputs":[],"source":["# ====== EXAMPLE DUMMY DATA (DELETE/REPLACE) ======\n","rng = np.random.default_rng(42)\n","n_subj = 8\n","n_per = 200\n","n_features = 30\n","n_targets = 5\n","X_list, Y_list, S_list = [], [], []\n","for s in range(n_subj):\n","    # subject-specific shifts\n","    shift = rng.normal(0, 3, size=n_features)\n","    Xs = rng.normal(0, 1, size=(n_per, n_features)) + shift\n","    W = rng.normal(0, 0.5, size=(n_features, n_targets))\n","    Ys = Xs @ W + rng.normal(0, 0.3, size=(n_per, n_targets))\n","    X_list.append(Xs)\n","    Y_list.append(Ys)\n","    S_list.append(np.full(n_per, s))\n","X = np.vstack(X_list)\n","Y = np.vstack(Y_list)\n","subjects = np.concatenate(S_list)\n","\n","# Wrap into DataFrames for convenience\n","X = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n","Y = pd.DataFrame(Y, columns=[f\"y{i}\" for i in range(Y.shape[1])])\n","\n","FEATURES_ARE_POWERS = False  # set True if your X are power-like (use log1p)\n","print(f\"X shape: {X.shape}; Y shape: {Y.shape}; subjects shape: {subjects.shape}\")\n"],"id":"lDZx7Nalg95w"},{"cell_type":"markdown","metadata":{"id":"Nde1r2H9g95w"},"source":["## Utilities: Metrics and Safe Conversions\n"],"id":"Nde1r2H9g95w"},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"Egp972cHg95w"},"outputs":[],"source":["def _to_numpy(a):\n","    if isinstance(a, pd.DataFrame) or isinstance(a, pd.Series):\n","        return a.values\n","    return np.asarray(a)\n","\n","def macro_pearsonr(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n","    # average Pearson r across targets; guard degenerate columns\n","    y_true = _to_numpy(y_true)\n","    y_pred = _to_numpy(y_pred)\n","    T = y_true.shape[1]\n","    rs = []\n","    for t in range(T):\n","        yt = y_true[:, t]\n","        yp = y_pred[:, t]\n","        if np.std(yt) < 1e-9 or np.std(yp) < 1e-9:\n","            rs.append(0.0)\n","        else:\n","            r, _ = pearsonr(yt, yp)\n","            rs.append(r)\n","    return float(np.mean(rs))\n","\n","def summarize_metrics(y_true, y_pred) -> Dict[str, float]:\n","    y_true = _to_numpy(y_true)\n","    y_pred = _to_numpy(y_pred)\n","    return {\n","        'R2': float(r2_score(y_true, y_pred, multioutput='uniform_average')),\n","        'MAE': float(mean_absolute_error(y_true, y_pred)),\n","        'PearsonR_macro': macro_pearsonr(y_true, y_pred)\n","    }\n"],"id":"Egp972cHg95w"},{"cell_type":"markdown","metadata":{"id":"gYfJE6K1g95w"},"source":["## Per-Subject Scaling without Leakage\n","We compute scalers **using train indices only**, *per subject*, then apply those scalers to both train and test samples of the **same subject**.\n"],"id":"gYfJE6K1g95w"},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"Eu2n8m-rg95x"},"outputs":[],"source":["from collections import defaultdict\n","\n","def scale_within_subject_train_stats(X: np.ndarray, subjects: np.ndarray, train_idx: np.ndarray, test_idx: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n","    X = _to_numpy(X).astype(float)\n","    subjects = _to_numpy(subjects)\n","    X_train, X_test = X[train_idx].copy(), X[test_idx].copy()\n","    subj_train_stats = {}\n","    # Fit stats per subject from TRAIN subset only\n","    for s in np.unique(subjects[train_idx]):\n","        s_mask = (subjects[train_idx] == s)\n","        Xs = X_train[s_mask]\n","        mean = Xs.mean(axis=0)\n","        std = Xs.std(axis=0, ddof=0)\n","        std[std < 1e-12] = 1.0  # avoid div by zero\n","        subj_train_stats[s] = (mean, std)\n","    # Transform train using its subject stats\n","    for s in np.unique(subjects[train_idx]):\n","        mean, std = subj_train_stats[s]\n","        mask = (subjects[train_idx] == s)\n","        X_train[mask] = (X_train[mask] - mean) / std\n","    # Transform test using the SAME subject stats (if subject unseen in train, fallback to global train stats)\n","    global_mean = X[train_idx].mean(axis=0)\n","    global_std = X[train_idx].std(axis=0, ddof=0)\n","    global_std[global_std < 1e-12] = 1.0\n","    for s in np.unique(subjects[test_idx]):\n","        if s in subj_train_stats:\n","            mean, std = subj_train_stats[s]\n","        else:\n","            mean, std = global_mean, global_std\n","        mask = (subjects[test_idx] == s)\n","        X_test[mask] = (X_test[mask] - mean) / std\n","    return X_train, X_test\n"],"id":"Eu2n8m-rg95x"},{"cell_type":"markdown","metadata":{"id":"buaLcwt4g95x"},"source":["## Y Preprocessing Pipeline (train-only fit)\n","Impute â†’ Standardize â†’ PCA; returns transformed arrays and fitted transformers.\n"],"id":"buaLcwt4g95x"},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"w9LmPVe4g95x"},"outputs":[],"source":["def preprocess_Y_train_test(Y_train, Y_test, n_components: int = None, var_ratio: float = 0.95):\n","    Y_train = _to_numpy(Y_train).astype(float)\n","    Y_test = _to_numpy(Y_test).astype(float)\n","    imp = SimpleImputer(strategy='median')\n","    Ytr_imp = imp.fit_transform(Y_train)\n","    Yte_imp = imp.transform(Y_test)\n","    y_scaler = StandardScaler()\n","    Ytr_sc = y_scaler.fit_transform(Ytr_imp)\n","    Yte_sc = y_scaler.transform(Yte_imp)\n","    if n_components is None:\n","        pca = PCA(n_components=var_ratio, svd_solver='full')\n","    else:\n","        pca = PCA(n_components=n_components)\n","    Ytr_z = pca.fit_transform(Ytr_sc)\n","    Yte_z = pca.transform(Yte_sc)\n","    return Ytr_z, Yte_z, {'imputer': imp, 'scaler': y_scaler, 'pca': pca}\n"],"id":"w9LmPVe4g95x"},{"cell_type":"markdown","metadata":{"id":"sqX4X7w6g95x"},"source":["## Main Evaluation Loop\n","Set `USE_LOSO = True` for Leave-One-Subject-Out; otherwise uses GroupKFold. Choose `PER_SUBJECT_SCALING` or `GLOBAL_SCALING`.\n"],"id":"sqX4X7w6g95x"},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"ub7wLuUmg95x"},"outputs":[],"source":["USE_LOSO = False            # set True for Leave-One-Subject-Out\n","N_SPLITS = 5               # ignored if USE_LOSO=True\n","PER_SUBJECT_SCALING = True # if False, uses global StandardScaler fit on train only\n","APPLY_LOG1P_TO_X = bool(FEATURES_ARE_POWERS)\n","\n","# Choose a model (you can try both)\n","MODEL = 'ridge'  # 'ridge' or 'rf'\n","Ridge_alphas = np.logspace(-3, 3, 13)\n","RF_params = dict(n_estimators=300, max_depth=None, random_state=0, n_jobs=-1)\n","\n","X_np = _to_numpy(X)\n","Y_np = _to_numpy(Y)\n","S = _to_numpy(subjects)\n","\n","if APPLY_LOG1P_TO_X:\n","    X_np = np.log1p(np.maximum(X_np, 0))\n","\n","unique_subj = np.unique(S)\n","if USE_LOSO:\n","    folds = [(np.where(S != s)[0], np.where(S == s)[0]) for s in unique_subj]\n","else:\n","    gkf = GroupKFold(n_splits=N_SPLITS)\n","    folds = list(gkf.split(X_np, Y_np, groups=S))\n","\n","metrics_all = []\n","for fold_id, (tr_idx, te_idx) in enumerate(folds, 1):\n","    Xtr, Xte = None, None\n","    if PER_SUBJECT_SCALING:\n","        Xtr, Xte = scale_within_subject_train_stats(X_np, S, tr_idx, te_idx)\n","    else:\n","        # Global scaler fit on train only\n","        xs = StandardScaler()\n","        Xtr = xs.fit_transform(X_np[tr_idx])\n","        Xte = xs.transform(X_np[te_idx])\n","\n","    Ytr_z, Yte_z, yproc = preprocess_Y_train_test(Y_np[tr_idx], Y_np[te_idx], n_components=None, var_ratio=0.95)\n","\n","    if MODEL == 'ridge':\n","        model = RidgeCV(alphas=Ridge_alphas, store_cv_values=False)\n","    elif MODEL == 'rf':\n","        model = RandomForestRegressor(**RF_params)\n","    else:\n","        raise ValueError(\"Unknown MODEL\")\n","\n","    model.fit(Xtr, Ytr_z)\n","    Yhat_z = model.predict(Xte)\n","    # Inverse-transform back to original Y space for metrics (optional). Here we metric in PCA space for stability.\n","    m = summarize_metrics(Yte_z, Yhat_z)\n","    m.update({'fold': fold_id, 'n_train': len(tr_idx), 'n_test': len(te_idx)})\n","    metrics_all.append(m)\n","    print(f\"Fold {fold_id}: R2={m['R2']:.3f}, MAE={m['MAE']:.3f}, r={m['PearsonR_macro']:.3f}\")\n","\n","df_metrics = pd.DataFrame(metrics_all)\n","print(\"\\nCV Summary:\")\n","print(df_metrics.describe().loc[['mean','std','min','max'], ['R2','MAE','PearsonR_macro']])\n"],"id":"ub7wLuUmg95x"},{"cell_type":"markdown","metadata":{"id":"cu_OZ65Sg95x"},"source":["## Compare Settings\n","You can re-run the loop with different switches (e.g., `USE_LOSO=True`, or `PER_SUBJECT_SCALING=False`) to see how results change."],"id":"cu_OZ65Sg95x"},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true,"id":"AYt7vU9lg95x"},"outputs":[],"source":["df_metrics.head()\n"],"id":"AYt7vU9lg95x"},{"cell_type":"markdown","metadata":{"id":"x-oQ4gdEg95y"},"source":["## Tips\n","- If your test set contains **unseen subjects**, prefer `PER_SUBJECT_SCALING=False` (global scaler fit on train) to avoid fitting any statistics on test subjects.\n","- If subjects appear in both train and test (e.g., temporal splits per subject), `PER_SUBJECT_SCALING=True` uses only **train windows of each subject** to compute that subject's scaler and applies it to the test windows of the same subject.\n","- Remove any **near-zero-variance** targets before PCA, or use `RobustScaler` if targets have heavy tails.\n","- Consider adding **time lags** or domain-specific features to strengthen Xâ†’Y mapping.\n"],"id":"x-oQ4gdEg95y"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.x"},"created":"2025-10-12T13:00:51.816615Z","colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}